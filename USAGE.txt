Snake Supervised ML - Quick Usage Guide
========================================

Prerequisites
-------------
1. Create virtual environment:
   python -m venv .venv

2. Activate virtual environment:
   Linux/Mac:  source .venv/bin/activate
   Windows:    .venv\Scripts\activate

3. Install dependencies:
   pip install numpy torch pygame


Workflow Options
----------------

OPTION A: Automated Training (Recommended)
-------------------------------------------

Step 1: Record Human Gameplay
   python play_pygame.py

   - Use arrow keys to control the snake
   - Play through 10-20 games
   - Data saved to: human_demos.npz

Step 2: Train Imitation Learning Model
   python train_imitation.py

   - Trains policy network on your gameplay
   - Model saved to: policy_imitation.pt

Step 3: Run Automated Iterative Training (WITH VISUALIZATION!)
   python iterative_train.py

   - Automatically runs 10 iterations of MC improvement
   - Auto-detects best policy each iteration
   - Tracks performance and saves history
   - Early stops if scores plateau
   - Model saved to: policy_mc.pt

   NEW FEATURES:
   - Evaluates human baseline performance
   - Saves model for each iteration (training_logs/models/)
   - Records best episode video for each iteration (training_logs/videos/)
   - Generates training progress plots (training_logs/training_progress.png)
   - Creates model comparison chart (training_logs/model_comparison.png)
   - Produces comprehensive text report (training_logs/training_report.txt)

Step 4: Watch Your Trained Agent
   python watch_agent.py data/policy_mc.pt

   - Visual playback in Pygame window
   - Press SPACE to restart, Q to quit


OPTION B: Manual Step-by-Step
------------------------------

Step 1-2: Same as Option A

Step 3: Generate Monte Carlo Self-Play Data
   python self_play.py

   - Agent plays using Monte Carlo rollouts
   - Auto-detects best policy (prefers policy_mc.pt)
   - Generates improved gameplay data
   - Data saved to: mc_demos.npz

Step 4: Fine-tune with Combined Data
   python train_mc.py

   - Combines human demos + MC demos
   - Fine-tunes policy network
   - Model saved to: policy_mc.pt

Step 5: Watch the Agent
   python watch_agent.py data/policy_mc.pt

Step 6: Iterate
   - Repeat steps 3-5 to improve performance
   - self_play.py will automatically use policy_mc.pt
   - Each iteration should improve the agent


Configuration
-------------
Key parameters you can adjust:

play_pygame.py:
  - num_games: number of games to record (default: 10)
  - grid_size: game board size (default: 10)
  - cell_size: pixel size of cells (default: 40)

train_imitation.py:
  - epochs: training epochs (default: 100)
  - batch_size: batch size (default: 32)
  - lr: learning rate (default: 1e-3)

self_play.py:
  - policy_path: None for auto-detect or specific path
  - num_episodes: number of self-play games (default: 50)
  - K: number of rollouts per action (default: 5, range: 5-20)
  - H: horizon length per rollout (default: 20, range: 10-50)

train_mc.py:
  - mc_weight: how much to weight MC data vs human (default: 2.0)
  - epochs: training epochs (default: 100)
  - lr: learning rate (default: 5e-4)

iterative_train.py:
  - num_iterations: number of training loops (default: 10)
  - mc_episodes: episodes per iteration (default: 50)
  - mc_K: rollouts per action (default: 5)
  - mc_H: horizon length (default: 20)
  - train_epochs: epochs per iteration (default: 100)
  - mc_weight: MC data weight (default: 2.0)
  - eval_episodes: evaluation episodes (default: 50)
  - record_videos: enable video recording (default: True)
  - video_episodes: episodes to find best for video (default: 10)

watch_agent.py:
  - policy_path: path to policy file
  - num_episodes: episodes to watch (default: 10)
  - fps: playback speed (default: 10)


Testing
-------
Run the full test suite (49 tests):
   python -m pytest tests/ -v

Run specific test file:
   python -m pytest tests/test_env.py -v
   python -m pytest tests/test_model.py -v
   python -m pytest tests/test_monte_carlo.py -v
   python -m pytest tests/test_self_play.py -v
   python -m pytest tests/test_iterative_train.py -v

Run single test:
   python -m pytest tests/test_env.py::TestSnakeEnv::test_clone -v


Visualization & Training Logs
------------------------------
After running iterative_train.py, you'll find:

training_logs/
  ├── training_progress.png       # Score improvement graph over iterations
  ├── model_comparison.png        # Bar chart comparing all models
  ├── training_report.txt         # Detailed text summary with stats
  ├── training_history.npz        # Raw data for custom analysis
  ├── models/                     # All saved models
  │   ├── policy_iter_0_imitation.pt
  │   ├── policy_iter_1_mc.pt
  │   ├── policy_iter_2_mc.pt
  │   └── ...
  └── videos/                     # Best episode videos (GIF format)
      ├── iter_0_imitation.gif
      ├── iter_1_mc.gif
      ├── iter_2_mc.gif
      └── ...

The progress plot shows:
  - Mean score ± std dev over iterations
  - Human baseline as reference line
  - Annotations for initial and final performance

The comparison chart shows:
  - Mean and max scores side-by-side for each iteration
  - Easy visual comparison of all models

Videos capture:
  - Best gameplay episode from each iteration
  - 10 FPS GIF format (easy to view anywhere)
  - Shows agent's gameplay strategy evolution

Required packages for visualization:
  pip install matplotlib imageio imageio[ffmpeg]


Tips
----
- Start with 10-20 human demo games
- Use automated training (iterative_train.py) for convenience
- Keep K and H small initially (K=5, H=20) for speed
- Increase K and H gradually as policy improves
- Watch scores improve across iterations
- If agent gets stuck, add more human demos with different strategies
- Use watch_agent.py to visually compare policies
- Training history is saved to data/training_history.npz
- Check training_logs/ folder for comprehensive visualization
- Videos show gameplay evolution - watch them in sequence!
- Compare specific models: python watch_agent.py training_logs/models/policy_iter_N_mc.pt
